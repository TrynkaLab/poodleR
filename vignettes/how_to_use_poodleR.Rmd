---
title: "How to use poodleR"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{how_to_use_poodleR}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```
## Scope of the package
poodleR (POOled Donor deconvolution by LEast square Regression) is a package that facilitates the estimation of individual donor proportions in pooled experiments using Whole Genome Sequencing (WGS) or Whole Exome Sequencing (WES) and donor genotype information. The deconvolution function itself needs two inputs: an allele frequency file created from the mapped sequencing reads from WGS/WES samples from pooled donors, and a genotype file that contains allele dosage information on each of the donors. These two files need to consistently refer to the same allele at each SNP position.

In this vignette we will explain how to obtain and filter the necessary input files for the deconvolution and reshape them in the correct format, and demonstrate the donor proportion deconvolution function itself.

```{r setup}
library(poodleR)
library(dplyr)
```
## Format of genotype VCF from donors
The genotype VCF to use as reference needs to have the following format:

```{r vcf}
load("vcf.rda")
original_vcf_rows = nrow(vcf@fix) # save this information for later
head(vcf@meta) # a few metadata lines
cbind(head(vcf@fix),head(vcf@gt)) # a few variants

```

The metadata lines are not important, but the information for all donors must be present in this file. You can have only the variants that have different alleles in at least one donor to maximize information while reducing file size.

## Format of the variant count file from WGS
The variant count file must be created with [bam-readcount](https://github.com/genome/bam-readcount), which looks like this:
```{r bam_readcount}
load("bam_readcount.rda")
head(bam_readcount)
```
NOTE: INCLUDE HERE A BAM-READCOUNT FILE WITH UNEQUAL NUMBER OF COLUMNS IN EACH ROW.
INCLUDE A VCF DESCRIPTION
SECOND SET OF POSSIBLE FILES:
INCLUDE A BCFTOOLS OUTPUT FILE DESCRIPTION
AND INCLUDE AN ALLELE DOSAGE FILE DESCRIPTION

## Donor deconvolution from genotype VCF and variant count file
Once you have the above files, first we need to load and process the VCF and the bam-readcount file to their common SNPs, and do any read count filtering. The latter is useful to subset the positions that will be used for deconvolution to those that have more information (more reads = more confidence calculating the non-reference allele frequency that is used in the deconvolution step). There is a function to do all these steps:
```{r calculate_nonRef_vector_and_genotype_df, eval=FALSE}
bam_path=system.file("extdata", "bam-readcount_example.tsv.gz", package = "poodleR")
v_path=system.file("extdata", "vcf_example.vcf.gz", package = "poodleR")

b_ma_list = calculate_nonRef_vector_and_genotype_df(bam_readcount_path =bam_path,
                                                    vcf_path = v_path,
                                                    min_read_overlap = 1,
                                                    b_estimate_path = NULL,
                                                    genotype_minor_allele_dos_path = NULL,
                                                    variants_retained_path = NULL
                                                    )

```
This returns a list with two data.tables with the following format:
```{r calculate_nonRef_vector_and_genotype_df_outputs, eval=FALSE}
b_ma_list$b_estimate_table
b_ma_list$ma_dosages
```
The deconvolution step needs the non-reference allele frequency from the pool, which we have calculated from the bam-readcount file and is under the column "b_estimate". We'll also need the minor allele dosage file, containing the same variants *in the same order*. 

These are the full columns of the data.table containing the allele frequency:
\n 

* **total_reads** Total number of reads mapping to that SNP.
* **A, C, T, G** Number of reads mapping to each of those bases.
* **b_estimate** The estimated non-reference allele frequency in the sequenced population at that position.
* **ALT** The alternative or non-reference allele.
* **CHROM_POS_REF** The ID created for that position, to match to the minor allele dosage genotype object.

\n 

The deconvolution step also needs the genotype information in minor allele dosages. These are the full columns of the data.table containing the minor allele dosage:
\n 

* **sample_name 1, 2, ..., n** Minor allele dosage for each sample. 
* **rn** Row names, CHROM_POS_REF (should match the column from the allele frequency object exactly).

\n 

We can see how many SNPs we have lost after all these filters:

```{r maf}

message("The proportion of SNPs retained from the input VCF was ", 
        round((nrow(b_ma_list$b_estimate_table) / original_vcf_rows)*100,digits = 2), " % \n")
message("There are  ", 
        nrow(b_ma_list$b_estimate_table)," variants remaining \n")

```

We can finally estimate the donor proportions from the population (here we'll call this `w_estimate`):

```{r w}

w_estimate = poodle::estimate_weights(b = b_ma_list$b_estimate_table$b_estimate,
                                      A = b_ma_list$ma_dosages)


```
The function first checks again for duplicate SNPs in both files, and then removes the SNPs that haven't been sampled by WGS and that have no counts in the MAF estimate file. The lower coverage you have by WGS, the higher this percentage will be and you'll use fewer SNPs at this step, reducing the accuracy of your donor proportion estimates.

\n

We'll now check the result, which is a named vector with individual proportions between 0-1:
```{r results}

w_estimate
sum(w_estimate) # should be 1

```


